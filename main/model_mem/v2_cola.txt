==================================================
Running config: v2_cola_adamw
==================================================
Loading dataset: cifar10...
Batches -> Train: 750, Val: 94, Test: 94
Using optimizer: adamw
========================================================================================================================
Layer (type:depth-idx)                                                 Output Shape              Param #
========================================================================================================================
ViTForImageClassification                                              [64, 10]                  --
├─ViTModel: 1-1                                                        [64, 17, 768]             --
│    └─ViTEmbeddings: 2-1                                              [64, 17, 768]             13,824
│    │    └─ViTPatchEmbeddings: 3-1                                    [64, 16, 768]             590,592
│    │    └─Dropout: 3-2                                               [64, 17, 768]             --
│    └─ViTEncoder: 2-2                                                 [64, 17, 768]             --
│    │    └─ModuleList: 3-3                                            --                        31,970,304
│    └─LayerNorm: 2-3                                                  [64, 17, 768]             1,536
├─Linear: 1-2                                                          [64, 10]                  7,690
========================================================================================================================
Total params: 32,583,946
Trainable params: 32,583,946
Non-trainable params: 0
Total mult-adds (Units.MEGABYTES): 613.02
========================================================================================================================
Input size (MB): 3.15
Forward/backward pass size (MB): 895.36
Params size (MB): 66.58
Estimated Total Size (MB): 965.08
========================================================================================================================

==================================================
Running config: v2_cola_glora
==================================================
Loading dataset: cifar10...
Batches -> Train: 750, Val: 94, Test: 94
Using optimizer: galore8
Using GaLoreAdamW8bit (param groups)
========================================================================================================================
Layer (type:depth-idx)                                                 Output Shape              Param #
========================================================================================================================
ViTForImageClassification                                              [64, 10]                  --
├─ViTModel: 1-1                                                        [64, 17, 768]             --
│    └─ViTEmbeddings: 2-1                                              [64, 17, 768]             13,824
│    │    └─ViTPatchEmbeddings: 3-1                                    [64, 16, 768]             590,592
│    │    └─Dropout: 3-2                                               [64, 17, 768]             --
│    └─ViTEncoder: 2-2                                                 [64, 17, 768]             --
│    │    └─ModuleList: 3-3                                            --                        31,970,304
│    └─LayerNorm: 2-3                                                  [64, 17, 768]             1,536
├─Linear: 1-2                                                          [64, 10]                  7,690
========================================================================================================================
Total params: 32,583,946
Trainable params: 32,583,946
Non-trainable params: 0
Total mult-adds (Units.MEGABYTES): 613.02
========================================================================================================================
Input size (MB): 3.15
Forward/backward pass size (MB): 895.36
Params size (MB): 66.58
Estimated Total Size (MB): 965.08
========================================================================================================================

==================================================
Running config: v2_cola_glora_layer
==================================================
Loading dataset: cifar10...
Batches -> Train: 750, Val: 94, Test: 94
Using optimizer: galore8_per_layer
Using GaLoreAdamW8bit *per layer*
========================================================================================================================
Layer (type:depth-idx)                                                 Output Shape              Param #
========================================================================================================================
ViTForImageClassification                                              [64, 10]                  --
├─ViTModel: 1-1                                                        [64, 17, 768]             --
│    └─ViTEmbeddings: 2-1                                              [64, 17, 768]             13,824
│    │    └─ViTPatchEmbeddings: 3-1                                    [64, 16, 768]             590,592
│    │    └─Dropout: 3-2                                               [64, 17, 768]             --
│    └─ViTEncoder: 2-2                                                 [64, 17, 768]             --
│    │    └─ModuleList: 3-3                                            --                        31,970,304
│    └─LayerNorm: 2-3                                                  [64, 17, 768]             1,536
├─Linear: 1-2                                                          [64, 10]                  7,690
========================================================================================================================
Total params: 32,583,946
Trainable params: 32,583,946
Non-trainable params: 0
Total mult-adds (Units.MEGABYTES): 613.02
========================================================================================================================
Input size (MB): 3.15
Forward/backward pass size (MB): 895.36
Params size (MB): 66.58
Estimated Total Size (MB): 965.08
========================================================================================================================
