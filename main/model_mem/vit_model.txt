==================================================
Running config: vit_adamw
==================================================
Loading dataset: cifar10...
Batches -> Train: 750, Val: 94, Test: 94
Using optimizer: adamw
=========================================================================================================
Layer (type:depth-idx)                                  Output Shape              Param #
=========================================================================================================
ViTForImageClassification                               [64, 10]                  --
├─ViTModel: 1-1                                         [64, 17, 768]             --
│    └─ViTEmbeddings: 2-1                               [64, 17, 768]             13,824
│    │    └─ViTPatchEmbeddings: 3-1                     [64, 16, 768]             590,592
│    │    └─Dropout: 3-2                                [64, 17, 768]             --
│    └─ViTEncoder: 2-2                                  [64, 17, 768]             --
│    │    └─ModuleList: 3-3                             --                        85,054,464
│    └─LayerNorm: 2-3                                   [64, 17, 768]             1,536
├─Linear: 1-2                                           [64, 10]                  7,690
=========================================================================================================
Total params: 85,668,106
Trainable params: 85,668,106
Non-trainable params: 0
Total mult-adds (Units.GIGABYTES): 6.05
=========================================================================================================
Input size (MB): 3.15
Forward/backward pass size (MB): 895.36
Params size (MB): 342.62
Estimated Total Size (MB): 1241.12
=========================================================================================================

==================================================
Running config: vit_glora
==================================================
Loading dataset: cifar10...
Batches -> Train: 750, Val: 94, Test: 94
Using optimizer: galore8
Using GaLoreAdamW8bit (param groups)
=========================================================================================================
Layer (type:depth-idx)                                  Output Shape              Param #
=========================================================================================================
ViTForImageClassification                               [64, 10]                  --
├─ViTModel: 1-1                                         [64, 17, 768]             --
│    └─ViTEmbeddings: 2-1                               [64, 17, 768]             13,824
│    │    └─ViTPatchEmbeddings: 3-1                     [64, 16, 768]             590,592
│    │    └─Dropout: 3-2                                [64, 17, 768]             --
│    └─ViTEncoder: 2-2                                  [64, 17, 768]             --
│    │    └─ModuleList: 3-3                             --                        85,054,464
│    └─LayerNorm: 2-3                                   [64, 17, 768]             1,536
├─Linear: 1-2                                           [64, 10]                  7,690
=========================================================================================================
Total params: 85,668,106
Trainable params: 85,668,106
Non-trainable params: 0
Total mult-adds (Units.GIGABYTES): 6.05
=========================================================================================================
Input size (MB): 3.15
Forward/backward pass size (MB): 895.36
Params size (MB): 342.62
Estimated Total Size (MB): 1241.12
=========================================================================================================

==================================================
Running config: vit_glora_layer
==================================================
Loading dataset: cifar10...
Batches -> Train: 750, Val: 94, Test: 94
Using optimizer: galore8_per_layer
Using GaLoreAdamW8bit *per layer*
=========================================================================================================
Layer (type:depth-idx)                                  Output Shape              Param #
=========================================================================================================
ViTForImageClassification                               [64, 10]                  --
├─ViTModel: 1-1                                         [64, 17, 768]             --
│    └─ViTEmbeddings: 2-1                               [64, 17, 768]             13,824
│    │    └─ViTPatchEmbeddings: 3-1                     [64, 16, 768]             590,592
│    │    └─Dropout: 3-2                                [64, 17, 768]             --
│    └─ViTEncoder: 2-2                                  [64, 17, 768]             --
│    │    └─ModuleList: 3-3                             --                        85,054,464
│    └─LayerNorm: 2-3                                   [64, 17, 768]             1,536
├─Linear: 1-2                                           [64, 10]                  7,690
=========================================================================================================
Total params: 85,668,106
Trainable params: 85,668,106
Non-trainable params: 0
Total mult-adds (Units.GIGABYTES): 6.05
=========================================================================================================
Input size (MB): 3.15
Forward/backward pass size (MB): 895.36
Params size (MB): 342.62
Estimated Total Size (MB): 1241.12
=========================================================================================================