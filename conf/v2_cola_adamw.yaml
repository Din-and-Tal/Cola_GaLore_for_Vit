    MODEL_NAME: 'v2_cola'
    OPTIMIZER_NAME: "adamw"  # 'adamw', 'galore8', 'galore8_per_layer'
    MEM_MEASURE: false
    SEED: 42
    NUM_WORKERS: 2 # TODO: check best value

    # ---------------------------
    # Data Settings
    # ---------------------------
    # Dataset choice: 'cifar10', 'cifar100', or 'imagefolder'
    DATASET_NAME: "cifar10"
    NUM_CLASSES: 10  # Change to 100 for CIFAR100

    # Data Split
    TRAIN_RATIO: 0.8
    VAL_RATIO: 0.1
    TEST_RATIO: 0.1

    # model saving/loading settings
    BASE_OUTPUT_DIR: "./saved_models"
    SAVE_MODEL: false
    LOAD_MODEL: false

    # ---------------------------
    # Model Hyperparameters
    # ---------------------------
    IMAGE_SIZE: 64
    PATCH_SIZE: 16
    NUM_CHANNELS: 3

    # ViT Architecture Config
    HIDDEN_SIZE: 768
    NUM_HIDDEN_LAYERS: 12
    NUM_ATTENTION_HEADS: 12
    INTERMEDIATE_SIZE: 3072  # usually 4x hidden_size

    # ---------------------------
    # Cola Parameters
    # ---------------------------
    COLA_RANK: 128
    COLA_RANK_RATIO: 0.25
    COLA_LR_ACT_TYPE: "gelu"

    # ---------------------------
    # Glora Parameters
    # ---------------------------
    GLORA_RANK: 128
    GLORA_UPDATE_PROJ_GAP: 200
    GLORA_SCALE: 0.25
    GLORA_PROJ_TYPE: "std"

    # ---------------------------
    # Training Hyperparameters
    # ---------------------------
    NUM_EPOCHS: 1
    BATCH_SIZE: 64
    LEARNING_RATE: 1e-3
    WEIGHT_DECAY: 0.05
    WARMUP_EPOCHS: 5
    MAX_GRAD_NORM: 1.0
    HIDDEN_DROPOUT_PROB: 0.0

    # ---------------------------
    # Scheduler hyperparameters
    # ---------------------------
    SCHEDULER_FIRST_CYCLE_STEPS: 200
    SCHEDULER_CYCLE_MULT: 1.0
    SCHEDULER_MAX_LR: 0.1
    SCHEDULER_MIN_LR: 0.001
    SCHEDULER_WARMUP_STEPS: 50
    SCHEDULER_GAMMA: 0.5

    # Mixed Precision (FP16)
    USE_AMP: false  

    # wandb configuration
    USE_WANDB: true
    WANDB_PROJECT_NAME: "efficient_vit_pretraining"
    WANDB_TEAM_NAME: "din-alon-technion-israel-institute-of-technology"
