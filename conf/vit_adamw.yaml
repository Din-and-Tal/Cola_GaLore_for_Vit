    MODEL_NAME: 'vit'
    OPTIMIZER_NAME: "adamw"  # 'adamw', 'galore8', 'galore8_per_layer'
    MEM_MEASURE: false
    SEED: 42
    NUM_WORKERS: 2 # TODO: check best value

    # ---------------------------
    # Data Settings
    # ---------------------------
    # Dataset choice: 'cifar10', 'cifar100', or 'imagefolder'
    DATASET_NAME: "cifar100"
    NUM_CLASSES: 100  # Change to 100 for CIFAR100

    # Data Split
    TRAIN_RATIO: 0.8
    VAL_RATIO: 0.1
    TEST_RATIO: 0.1

    # model saving/loading settings
    BASE_OUTPUT_DIR: "./saved_models"
    SAVE_MODEL: false
    LOAD_MODEL: false

    # ---------------------------
    # Model Hyperparameters
    # ---------------------------
    IMAGE_SIZE: 32
    PATCH_SIZE: 8
    NUM_CHANNELS: 3

    # ViT Architecture
    HIDDEN_SIZE: 192        # Tiny=192 | Small=384 | Base=768 | Large=1024 | Huge=1280
    NUM_HIDDEN_LAYERS: 12   # Tiny=12 | Small=12 | Base=12 | Large=24 | Huge=32
    NUM_ATTENTION_HEADS: 3  # Tiny=3  | Small=6  | Base=12 | Large=16 | Huge=16
    INTERMEDIATE_SIZE: 768   # usually 4×HIDDEN_SIZE → Tiny=768 | Small=1536 | Base=3072 | Large=4096 | Huge=5120

    # Approximate parameter count (for reference)
    # Tiny ≈ 5.7M | Small ≈ 22M | Base ≈ 86M | Large ≈ 307M | Huge ≈ 632M

    # ---------------------------
    # Profiling Options
    # ---------------------------
    PROFILE_MEMORY: true

    # ---------------------------
    # Cola Parameters
    # ---------------------------
    COLA_RANK: 0 # TODO: find better solution for invalid parameters
    COLA_RANK_RATIO: 0.25
    COLA_LR_ACT_TYPE: "gelu"

    # ---------------------------
    # Glora Parameters
    # ---------------------------
    GLORA_RANK: 128
    GLORA_UPDATE_PROJ_GAP: 200
    GLORA_SCALE: 0.25
    GLORA_PROJ_TYPE: "std"

    # ---------------------------
    # Training Hyperparameters
    # ---------------------------
    NUM_EPOCHS: 10
    BATCH_SIZE: 256
    LEARNING_RATE: 1e-3
    WEIGHT_DECAY: 0.05
    WARMUP_EPOCHS: 5
    MAX_GRAD_NORM: 1.0
    HIDDEN_DROPOUT_PROB: 0.0

    # ---------------------------
    # Scheduler hyperparameters
    # ---------------------------
    SCHEDULER_FIRST_CYCLE_STEPS: 200
    SCHEDULER_CYCLE_MULT: 1.0
    SCHEDULER_MAX_LR: 0.1
    SCHEDULER_MIN_LR: 0.001
    SCHEDULER_WARMUP_STEPS: 50
    SCHEDULER_GAMMA: 0.5

    # Mixed Precision (FP16)
    USE_AMP: false  

    # wandb configuration
    USE_WANDB: false
    WANDB_PROJECT_NAME: "vit-cifar100"
    WANDB_TEAM_NAME: "din-alon-technion-israel-institute-of-technology"


