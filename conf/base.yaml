defaults:
  - _self_
  - optuna

use_profiler: false
use_bf16: true
use_activation_checkpointing: false

optimizer_name: adamw #adamw , adamw8, galore, galore_layer
use_wandb: true
use_amp: false # TODO: integrate with galore
use_optuna: false

full_train: true
size: tiny
num_workers: 4

cola_rank_ratio: 0.25
cola_act: "silu"
cola_use_checkpointing: true
cola_use_intermediate_rank_scale: false

galore_rank: 128
galore_update_proj_gap: 200
galore_scale: 0.25
galore_proj_type: "std"

total_steps: -1 #this will be overriden in trainer setup

early_stopping_patience: -1

num_epochs: 150
batch_size: 512
weight_decay: 0.06 # 0.037343324638226455
max_grad_norm: 1.0

scheduler_cycle_mult: 1.0
scheduler_max_lr: 3e-4  #0.00020545522334013431
scheduler_min_lr: 1e-6
scheduler_warmup_steps: 5
scheduler_gamma: 1
scheduler_warmup_pct: 0.02

hidden_dropout_prob: 0.2 #0.0910218284106628

label_smoothing: 0.1 #0.09740441812870372

use_mixup: true
mixup_alpha: 0.2 #0.27067234802367746
use_cutmix: true
cutmix_alpha: 0.2 #0.06735967157072321
mix_prob: 0.2 #0.5440847747941563

# Augmentation
aug_crop_min_scale: 0.6
aug_crop_max_scale: 1.0
aug_crop_min_ratio: 0.75
aug_crop_max_ratio: 1.33
aug_rand_num_ops: 2
aug_rand_magnitude: 9
aug_erase_prob: 0.1
aug_erase_min_scale: 0.02
aug_erase_max_scale: 0.33
aug_erase_min_ratio: 0.3
aug_erase_max_ratio: 3.3

seed: 42

image_size: 64
patch_size: 8
num_channels: 3

#tiny_imagenet, 200 classes
# imagenette2, 10 classes
dataset_name: "imagenette2"
num_classes: 10

wandb_project_name: "no_project_name"
wandb_team_name: "din-alon-technion-israel-institute-of-technology"

config_name: ${hydra:job.config_name}
verbose: true

# model sizes --------------------------------------------
vit_presents:
  tiny:
    hidden_size: 192
    num_hidden_layers: 12
    num_attention_heads: 3
    intermediate_size: 768

  small:
    hidden_size: 384
    num_hidden_layers: 12
    num_attention_heads: 6
    intermediate_size: 1536

  base:
    hidden_size: 768
    num_hidden_layers: 12
    num_attention_heads: 12
    intermediate_size: 3072

  large:
    hidden_size: 1024
    num_hidden_layers: 24
    num_attention_heads: 16
    intermediate_size: 4096

  huge:
    hidden_size: 1280
    num_hidden_layers: 32
    num_attention_heads: 16
    intermediate_size: 5120

hidden_size: ${vit_presents.${size}.hidden_size}
num_hidden_layers: ${vit_presents.${size}.num_hidden_layers}
num_attention_heads: ${vit_presents.${size}.num_attention_heads}
intermediate_size: ${vit_presents.${size}.intermediate_size}
