defaults:
  - _self_

use_wandb: true

full_train: true
debug_data_scale: 0.001
size: tiny
profile_memory: false


vit_presents:
  tiny:
    hidden_size: 192
    num_hidden_layers: 12
    num_attention_heads: 3
    intermediate_size: 768

  small:
    hidden_size: 384
    num_hidden_layers: 12
    num_attention_heads: 6
    intermediate_size: 1536

  base:
    hidden_size: 768
    num_hidden_layers: 12
    num_attention_heads: 12
    intermediate_size: 3072

  large:
    hidden_size: 1024
    num_hidden_layers: 24
    num_attention_heads: 16
    intermediate_size: 4096

  huge:
    hidden_size: 1280
    num_hidden_layers: 32
    num_attention_heads: 16
    intermediate_size: 5120
    total_params: 632_000_000

hidden_size: ${vit_presents.${size}.hidden_size}
num_hidden_layers: ${vit_presents.${size}.num_hidden_layers}
num_attention_heads: ${vit_presents.${size}.num_attention_heads}
intermediate_size: ${vit_presents.${size}.intermediate_size}

seed: 42
num_workers: 4

dataset_name: "cifar100"
num_classes: 100  # Change to 100 for CIFAR100

train_ratio: 0.8
val_ratio: 0.1
test_ratio: 0.1

base_output_dir: "./saved_models"
save_model: false
load_model: false

image_size: 32
patch_size: 4
num_channels: 3

cola_rank: 128
cola_rank_ratio: 0.25
cola_lr_act_type: "gelu"

glora_rank: 128
glora_update_proj_gap: 200
glora_scale: 0.25
glora_proj_type: "std"

num_epochs: 1
eval_frequency: 5
batch_size: 64
learning_rate: 1e-3
weight_decay: 0.05
warmup_epochs: 5
max_grad_norm: 1.0
hidden_dropout_prob: 0.0

scheduler_first_cycle_steps: 200
scheduler_cycle_mult: 1.0
scheduler_max_lr: 0.1
scheduler_min_lr: 0.001
scheduler_warmup_steps: 50
scheduler_gamma: 0.5

wandb_project_name: "efficient_vit_pretraining"
wandb_team_name: "din-alon-technion-israel-institute-of-technology"

config_name: ${hydra:job.config_name}
