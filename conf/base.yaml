#TODO: organize better / clean up unused params

defaults:
  - _self_
  - optuna

use_optuna: true
use_wandb: true

full_train: true
size: tiny
profile_memory: false
num_workers: 4
use_amp: true
compile_model: true

seed: 42

dataset_name: "tiny_imagenet"
num_classes: 200

base_output_dir: "./saved_models"
save_model: false
load_model:  #TODO : DELETE

image_size: 64
patch_size: 8
num_channels: 3

cola_rank: 128
cola_rank_ratio: 0.25
cola_lr_act_type: "gelu"

glora_rank: 128
glora_update_proj_gap: 200
glora_scale: 0.25
glora_proj_type: "std"

num_epochs: 300
batch_size: 512
weight_decay: 0.06
warmup_epochs: 5
max_grad_norm: 1.0

hidden_dropout_prob: 0.2
label_smoothing: 0.1
scheduler_first_cycle_steps: 300
scheduler_cycle_mult: 1.0
scheduler_max_lr: 3e-4 # this value is initialized in all optimizers
scheduler_min_lr: 1e-6
scheduler_warmup_steps: 5
scheduler_gamma: 1

use_mixup: true
mixup_alpha: 0.2   # small → won’t kill train_acc

use_cutmix: true
cutmix_alpha: 0.2  # small
mix_prob: 0.2      # prob to apply mixup/cutmix on a batch

wandb_project_name: "tiny-imagenet"
wandb_team_name: "din-alon-technion-israel-institute-of-technology"

config_name: ${hydra:job.config_name}

vit_presents:
  tiny:
    hidden_size: 192
    num_hidden_layers: 12
    num_attention_heads: 3
    intermediate_size: 768

  small:
    hidden_size: 384
    num_hidden_layers: 12
    num_attention_heads: 6
    intermediate_size: 1536

  base:
    hidden_size: 768
    num_hidden_layers: 12
    num_attention_heads: 12
    intermediate_size: 3072

  large:
    hidden_size: 1024
    num_hidden_layers: 24
    num_attention_heads: 16
    intermediate_size: 4096

  huge:
    hidden_size: 1280
    num_hidden_layers: 32
    num_attention_heads: 16
    intermediate_size: 5120
    total_params: 632_000_000

hidden_size: ${vit_presents.${size}.hidden_size}
num_hidden_layers: ${vit_presents.${size}.num_hidden_layers}
num_attention_heads: ${vit_presents.${size}.num_attention_heads}
intermediate_size: ${vit_presents.${size}.intermediate_size}
