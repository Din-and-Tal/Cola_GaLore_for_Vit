    defaults:
    - _self_
   
    full_train: true
    debug_data_scale: 0.1
    PROFILE_MEMORY: false
    size: tiny

    vit_presents:
      tiny:
         HIDDEN_SIZE: 192
         NUM_HIDDEN_LAYERS: 12
         NUM_ATTENTION_HEADS: 3
         INTERMEDIATE_SIZE: 768

      small:
         HIDDEN_SIZE: 384
         NUM_HIDDEN_LAYERS: 12
         NUM_ATTENTION_HEADS: 6
         INTERMEDIATE_SIZE: 1536

      base:
         HIDDEN_SIZE: 768
         NUM_HIDDEN_LAYERS: 12
         NUM_ATTENTION_HEADS: 12
         INTERMEDIATE_SIZE: 3072

      large:
         HIDDEN_SIZE: 1024
         NUM_HIDDEN_LAYERS: 24
         NUM_ATTENTION_HEADS: 16
         INTERMEDIATE_SIZE: 4096

    HIDDEN_SIZE: ${vit_presents.${size}.HIDDEN_SIZE}
    NUM_HIDDEN_LAYERS: ${vit_presents.${size}.NUM_HIDDEN_LAYERS}
    NUM_ATTENTION_HEADS: ${vit_presents.${size}.NUM_ATTENTION_HEADS}
    INTERMEDIATE_SIZE: ${vit_presents.${size}.INTERMEDIATE_SIZE}

    SEED: 42
    NUM_WORKERS: 2 # TODO: check best value

    DATASET_NAME: "cifar100"
    NUM_CLASSES: 100  # Change to 100 for CIFAR100

    TRAIN_RATIO: 0.8
    VAL_RATIO: 0.1
    TEST_RATIO: 0.1

    BASE_OUTPUT_DIR: "./saved_models"
    SAVE_MODEL: false
    LOAD_MODEL: false

    IMAGE_SIZE: 32
    PATCH_SIZE: 4
    NUM_CHANNELS: 3

    COLA_RANK: 128
    COLA_RANK_RATIO: 0.25
    COLA_LR_ACT_TYPE: "gelu"

    GLORA_RANK: 128
    GLORA_UPDATE_PROJ_GAP: 200
    GLORA_SCALE: 0.25
    GLORA_PROJ_TYPE: "std"

    NUM_EPOCHS: 1
    EVAL_FREQUENCY: 5
    BATCH_SIZE: 64
    LEARNING_RATE: 1e-3
    WEIGHT_DECAY: 0.05
    WARMUP_EPOCHS: 5
    MAX_GRAD_NORM: 1.0
    HIDDEN_DROPOUT_PROB: 0.0

    SCHEDULER_FIRST_CYCLE_STEPS: 200
    SCHEDULER_CYCLE_MULT: 1.0
    SCHEDULER_MAX_LR: 0.1
    SCHEDULER_MIN_LR: 0.001
    SCHEDULER_WARMUP_STEPS: 50
    SCHEDULER_GAMMA: 0.5

    USE_WANDB: true
    WANDB_PROJECT_NAME: "efficient_vit_pretraining"
    WANDB_TEAM_NAME: "din-alon-technion-israel-institute-of-technology"

    config_name: ${hydra:job.config_name }
